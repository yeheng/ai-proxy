# AI Proxy Configuration Example
# Copy this file to config.toml and update with your actual API keys and settings
# This configuration file demonstrates all available options with their default values

# ============================================================================
# Server Configuration
# ============================================================================
[server]
# Server binding address (use "0.0.0.0" for all interfaces in production)
host = "127.0.0.1"

# Server port number
port = 3000

# Request timeout in seconds (1-300 seconds)
request_timeout_seconds = 30

# Maximum request size in bytes (1 byte - 100MB)
max_request_size_bytes = 1048576  # 1MB

# ============================================================================
# AI Provider Configurations
# Each provider section defines an AI service provider with its specific settings
# ============================================================================

[providers.gemini]
# Google Gemini API configuration
api_key = "your-gemini-api-key-here"
api_base = "https://generativelanguage.googleapis.com/v1beta/models/"

# Available models for this provider
models = [
    "gemini-1.5-pro-latest",
    "gemini-1.5-flash-latest",
    "gemini-pro",
    "gemini-pro-vision"
]

# Provider-specific timeout in seconds (1-600 seconds)
timeout_seconds = 60

# Maximum retry attempts for failed requests (0-10)
max_retries = 3

# Whether this provider is enabled
enabled = true

# Optional rate limiting configuration
[providers.gemini.rate_limit]
requests_per_minute = 60
burst_size = 10

[providers.openai]
# OpenAI API configuration
api_key = "your-openai-api-key-here"
api_base = "https://api.openai.com/v1/"

# Available models for this provider
models = [
    "gpt-4",
    "gpt-4-turbo-preview",
    "gpt-4-vision-preview",
    "gpt-3.5-turbo",
    "gpt-3.5-turbo-16k"
]

# Provider-specific settings
timeout_seconds = 60
max_retries = 3
enabled = true

# Rate limiting for OpenAI
[providers.openai.rate_limit]
requests_per_minute = 100
burst_size = 20

[providers.anthropic]
# Anthropic Claude API configuration
api_key = "your-anthropic-api-key-here"
api_base = "https://api.anthropic.com/v1/"

# Available Claude models
models = [
    "claude-3-5-sonnet-20241022",
    "claude-3-5-haiku-20241022",
    "claude-3-opus-20240229",
    "claude-3-sonnet-20240229",
    "claude-3-haiku-20240307"
]

# Provider-specific settings
timeout_seconds = 60
max_retries = 3
enabled = true

# Rate limiting for Anthropic
[providers.anthropic.rate_limit]
requests_per_minute = 50
burst_size = 5

# ============================================================================
# Logging Configuration
# ============================================================================
[logging]
# Log level: "trace", "debug", "info", "warn", "error"
level = "info"

# Log format: "json", "pretty", "compact"
format = "json"

# Whether to log incoming requests
log_requests = true

# Whether to log outgoing responses (may contain sensitive data)
log_responses = false

# ============================================================================
# Security Configuration
# ============================================================================
[security]
# API keys for client authentication (optional)
# If empty, no authentication is required
api_keys = [
    # "your-client-api-key-1",
    # "your-client-api-key-2"
]

# Enable CORS support
cors_enabled = true

# Allowed origins for CORS (empty means allow all)
allowed_origins = [
    # "https://your-frontend-domain.com",
    # "http://localhost:3000"
]

# Enable global rate limiting
rate_limit_enabled = false

# ============================================================================
# Performance Configuration
# ============================================================================
[performance]
# HTTP connection pool size per provider (1-1000)
connection_pool_size = 10

# Keep-alive timeout for connections in seconds (1-3600)
keep_alive_timeout_seconds = 60

# Maximum concurrent requests the server can handle (1-10000)
max_concurrent_requests = 100

# ============================================================================
# Environment Variable Overrides
# ============================================================================
# You can override any configuration value using environment variables
# with the AI_PROXY_ prefix. Examples:
#
# Server settings:
# AI_PROXY_SERVER_HOST=0.0.0.0
# AI_PROXY_SERVER_PORT=8080
# AI_PROXY_SERVER_REQUEST_TIMEOUT_SECONDS=45
#
# Provider API keys:
# AI_PROXY_PROVIDERS_GEMINI_API_KEY=your-gemini-key
# AI_PROXY_PROVIDERS_OPENAI_API_KEY=your-openai-key
# AI_PROXY_PROVIDERS_ANTHROPIC_API_KEY=your-anthropic-key
#
# Provider settings:
# AI_PROXY_PROVIDERS_OPENAI_ENABLED=false
# AI_PROXY_PROVIDERS_GEMINI_TIMEOUT_SECONDS=120
#
# Logging settings:
# AI_PROXY_LOGGING_LEVEL=debug
# AI_PROXY_LOGGING_FORMAT=pretty
#
# Security settings:
# AI_PROXY_SECURITY_CORS_ENABLED=false
# AI_PROXY_SECURITY_RATE_LIMIT_ENABLED=true
#
# Performance settings:
# AI_PROXY_PERFORMANCE_CONNECTION_POOL_SIZE=20
# AI_PROXY_PERFORMANCE_MAX_CONCURRENT_REQUESTS=200

# ============================================================================
# Quick Start Guide
# ============================================================================
# 1. Copy this file to config.toml
# 2. Replace "your-*-api-key-here" with your actual API keys
# 3. Adjust server host/port if needed
# 4. Run: cargo run
# 5. Test: curl -X POST http://localhost:3000/v1/messages \
#          -H "Content-Type: application/json" \
#          -d '{"model":"gpt-3.5-turbo","messages":[{"role":"user","content":"Hello!"}],"max_tokens":100}'

# ============================================================================
# Production Deployment Notes
# ============================================================================
# - Set host to "0.0.0.0" to accept connections from all interfaces
# - Use environment variables for sensitive data like API keys
# - Enable authentication by setting security.api_keys
# - Configure appropriate rate limits for your use case
# - Monitor logs and adjust logging levels as needed
# - Consider using a reverse proxy (nginx, traefik) for SSL termination
# - Set up health check monitoring on /health endpoint